{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lYDan1hNWbs9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset MNIST dan CIFAR-10"
      ],
      "metadata": {
        "id": "weWNWVLeXzan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST\n",
        "transform_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset_mnist = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "trainloader_mnist = torch.utils.data.DataLoader(trainset_mnist, batch_size=32, shuffle=True)\n",
        "\n",
        "testset_mnist = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "testloader_mnist = torch.utils.data.DataLoader(testset_mnist, batch_size=32, shuffle=False)\n",
        "\n",
        "#CIFAR\n",
        "transform_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset_cifar = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
        "trainloader_cifar = torch.utils.data.DataLoader(trainset_cifar, batch_size=32, shuffle=True)\n",
        "\n",
        "testset_cifar = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
        "testloader_cifar = torch.utils.data.DataLoader(testset_cifar, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZjeNzI7IXqRt",
        "outputId": "df090020-788f-49d7-93fd-462736d23aa4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Model"
      ],
      "metadata": {
        "id": "Sal4bl3RYTj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AYnvU5QIYKBJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_cifar(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN_cifar, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OPcV0Udafmvu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "ou_1gLA0YWO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, trainloader, num_epochs=5, learning_rate=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
        "                running_loss = 0.0"
      ],
      "metadata": {
        "id": "To7DqPR5YZbM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi model"
      ],
      "metadata": {
        "id": "lR8x5vIHYhoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Akurasi: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "mBf_CORbYi41"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST"
      ],
      "metadata": {
        "id": "iEuLv9uBZAU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mnist = CNN(num_classes=10)\n",
        "train_model(model_mnist, trainloader_mnist)\n",
        "evaluate_model(model_mnist, testloader_mnist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzeC_beRZLyY",
        "outputId": "17370de1-53cb-4dc5-f979-4e103d63f73b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.864\n",
            "Epoch 1, Batch 200, Loss: 0.227\n",
            "Epoch 1, Batch 300, Loss: 0.142\n",
            "Epoch 1, Batch 400, Loss: 0.134\n",
            "Epoch 1, Batch 500, Loss: 0.122\n",
            "Epoch 1, Batch 600, Loss: 0.115\n",
            "Epoch 1, Batch 700, Loss: 0.098\n",
            "Epoch 1, Batch 800, Loss: 0.082\n",
            "Epoch 1, Batch 900, Loss: 0.086\n",
            "Epoch 1, Batch 1000, Loss: 0.070\n",
            "Epoch 1, Batch 1100, Loss: 0.092\n",
            "Epoch 1, Batch 1200, Loss: 0.059\n",
            "Epoch 1, Batch 1300, Loss: 0.063\n",
            "Epoch 1, Batch 1400, Loss: 0.075\n",
            "Epoch 1, Batch 1500, Loss: 0.071\n",
            "Epoch 1, Batch 1600, Loss: 0.053\n",
            "Epoch 1, Batch 1700, Loss: 0.061\n",
            "Epoch 1, Batch 1800, Loss: 0.066\n",
            "Epoch 2, Batch 100, Loss: 0.060\n",
            "Epoch 2, Batch 200, Loss: 0.034\n",
            "Epoch 2, Batch 300, Loss: 0.046\n",
            "Epoch 2, Batch 400, Loss: 0.049\n",
            "Epoch 2, Batch 500, Loss: 0.043\n",
            "Epoch 2, Batch 600, Loss: 0.043\n",
            "Epoch 2, Batch 700, Loss: 0.045\n",
            "Epoch 2, Batch 800, Loss: 0.037\n",
            "Epoch 2, Batch 900, Loss: 0.044\n",
            "Epoch 2, Batch 1000, Loss: 0.037\n",
            "Epoch 2, Batch 1100, Loss: 0.040\n",
            "Epoch 2, Batch 1200, Loss: 0.042\n",
            "Epoch 2, Batch 1300, Loss: 0.054\n",
            "Epoch 2, Batch 1400, Loss: 0.048\n",
            "Epoch 2, Batch 1500, Loss: 0.035\n",
            "Epoch 2, Batch 1600, Loss: 0.032\n",
            "Epoch 2, Batch 1700, Loss: 0.039\n",
            "Epoch 2, Batch 1800, Loss: 0.050\n",
            "Epoch 3, Batch 100, Loss: 0.028\n",
            "Epoch 3, Batch 200, Loss: 0.030\n",
            "Epoch 3, Batch 300, Loss: 0.029\n",
            "Epoch 3, Batch 400, Loss: 0.017\n",
            "Epoch 3, Batch 500, Loss: 0.033\n",
            "Epoch 3, Batch 600, Loss: 0.036\n",
            "Epoch 3, Batch 700, Loss: 0.017\n",
            "Epoch 3, Batch 800, Loss: 0.025\n",
            "Epoch 3, Batch 900, Loss: 0.032\n",
            "Epoch 3, Batch 1000, Loss: 0.028\n",
            "Epoch 3, Batch 1100, Loss: 0.023\n",
            "Epoch 3, Batch 1200, Loss: 0.032\n",
            "Epoch 3, Batch 1300, Loss: 0.030\n",
            "Epoch 3, Batch 1400, Loss: 0.031\n",
            "Epoch 3, Batch 1500, Loss: 0.031\n",
            "Epoch 3, Batch 1600, Loss: 0.034\n",
            "Epoch 3, Batch 1700, Loss: 0.037\n",
            "Epoch 3, Batch 1800, Loss: 0.027\n",
            "Epoch 4, Batch 100, Loss: 0.019\n",
            "Epoch 4, Batch 200, Loss: 0.013\n",
            "Epoch 4, Batch 300, Loss: 0.013\n",
            "Epoch 4, Batch 400, Loss: 0.020\n",
            "Epoch 4, Batch 500, Loss: 0.026\n",
            "Epoch 4, Batch 600, Loss: 0.015\n",
            "Epoch 4, Batch 700, Loss: 0.029\n",
            "Epoch 4, Batch 800, Loss: 0.019\n",
            "Epoch 4, Batch 900, Loss: 0.016\n",
            "Epoch 4, Batch 1000, Loss: 0.021\n",
            "Epoch 4, Batch 1100, Loss: 0.030\n",
            "Epoch 4, Batch 1200, Loss: 0.021\n",
            "Epoch 4, Batch 1300, Loss: 0.021\n",
            "Epoch 4, Batch 1400, Loss: 0.019\n",
            "Epoch 4, Batch 1500, Loss: 0.019\n",
            "Epoch 4, Batch 1600, Loss: 0.020\n",
            "Epoch 4, Batch 1700, Loss: 0.024\n",
            "Epoch 4, Batch 1800, Loss: 0.031\n",
            "Epoch 5, Batch 100, Loss: 0.019\n",
            "Epoch 5, Batch 200, Loss: 0.012\n",
            "Epoch 5, Batch 300, Loss: 0.020\n",
            "Epoch 5, Batch 400, Loss: 0.012\n",
            "Epoch 5, Batch 500, Loss: 0.016\n",
            "Epoch 5, Batch 600, Loss: 0.017\n",
            "Epoch 5, Batch 700, Loss: 0.010\n",
            "Epoch 5, Batch 800, Loss: 0.015\n",
            "Epoch 5, Batch 900, Loss: 0.011\n",
            "Epoch 5, Batch 1000, Loss: 0.013\n",
            "Epoch 5, Batch 1100, Loss: 0.025\n",
            "Epoch 5, Batch 1200, Loss: 0.022\n",
            "Epoch 5, Batch 1300, Loss: 0.021\n",
            "Epoch 5, Batch 1400, Loss: 0.017\n",
            "Epoch 5, Batch 1500, Loss: 0.017\n",
            "Epoch 5, Batch 1600, Loss: 0.014\n",
            "Epoch 5, Batch 1700, Loss: 0.013\n",
            "Epoch 5, Batch 1800, Loss: 0.022\n",
            "Akurasi: 99.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR-10"
      ],
      "metadata": {
        "id": "sxSrBhkMZNbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cifar = CNN_cifar(num_classes=10)\n",
        "train_model(model_cifar, trainloader_cifar)\n",
        "evaluate_model(model_cifar, testloader_cifar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqdc_HqyZPek",
        "outputId": "5d45fc20-3562-4557-828a-c1c79b59f6d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.966\n",
            "Epoch 1, Batch 200, Loss: 1.641\n",
            "Epoch 1, Batch 300, Loss: 1.526\n",
            "Epoch 1, Batch 400, Loss: 1.424\n",
            "Epoch 1, Batch 500, Loss: 1.373\n",
            "Epoch 1, Batch 600, Loss: 1.341\n",
            "Epoch 1, Batch 700, Loss: 1.305\n",
            "Epoch 1, Batch 800, Loss: 1.218\n",
            "Epoch 1, Batch 900, Loss: 1.235\n",
            "Epoch 1, Batch 1000, Loss: 1.185\n",
            "Epoch 1, Batch 1100, Loss: 1.179\n",
            "Epoch 1, Batch 1200, Loss: 1.121\n",
            "Epoch 1, Batch 1300, Loss: 1.054\n",
            "Epoch 1, Batch 1400, Loss: 1.081\n",
            "Epoch 1, Batch 1500, Loss: 1.077\n",
            "Epoch 2, Batch 100, Loss: 1.002\n",
            "Epoch 2, Batch 200, Loss: 1.001\n",
            "Epoch 2, Batch 300, Loss: 0.938\n",
            "Epoch 2, Batch 400, Loss: 0.928\n",
            "Epoch 2, Batch 500, Loss: 0.942\n",
            "Epoch 2, Batch 600, Loss: 0.902\n",
            "Epoch 2, Batch 700, Loss: 0.947\n",
            "Epoch 2, Batch 800, Loss: 0.937\n",
            "Epoch 2, Batch 900, Loss: 0.933\n",
            "Epoch 2, Batch 1000, Loss: 0.918\n",
            "Epoch 2, Batch 1100, Loss: 0.897\n",
            "Epoch 2, Batch 1200, Loss: 0.878\n",
            "Epoch 2, Batch 1300, Loss: 0.895\n",
            "Epoch 2, Batch 1400, Loss: 0.904\n",
            "Epoch 2, Batch 1500, Loss: 0.869\n",
            "Epoch 3, Batch 100, Loss: 0.794\n",
            "Epoch 3, Batch 200, Loss: 0.770\n",
            "Epoch 3, Batch 300, Loss: 0.779\n",
            "Epoch 3, Batch 400, Loss: 0.802\n",
            "Epoch 3, Batch 500, Loss: 0.795\n",
            "Epoch 3, Batch 600, Loss: 0.755\n",
            "Epoch 3, Batch 700, Loss: 0.798\n",
            "Epoch 3, Batch 800, Loss: 0.745\n",
            "Epoch 3, Batch 900, Loss: 0.773\n",
            "Epoch 3, Batch 1000, Loss: 0.721\n",
            "Epoch 3, Batch 1100, Loss: 0.770\n",
            "Epoch 3, Batch 1200, Loss: 0.756\n",
            "Epoch 3, Batch 1300, Loss: 0.784\n",
            "Epoch 3, Batch 1400, Loss: 0.761\n",
            "Epoch 3, Batch 1500, Loss: 0.765\n",
            "Epoch 4, Batch 100, Loss: 0.622\n",
            "Epoch 4, Batch 200, Loss: 0.639\n",
            "Epoch 4, Batch 300, Loss: 0.677\n",
            "Epoch 4, Batch 400, Loss: 0.680\n",
            "Epoch 4, Batch 500, Loss: 0.650\n",
            "Epoch 4, Batch 600, Loss: 0.659\n",
            "Epoch 4, Batch 700, Loss: 0.665\n",
            "Epoch 4, Batch 800, Loss: 0.657\n",
            "Epoch 4, Batch 900, Loss: 0.665\n",
            "Epoch 4, Batch 1000, Loss: 0.668\n",
            "Epoch 4, Batch 1100, Loss: 0.671\n",
            "Epoch 4, Batch 1200, Loss: 0.664\n",
            "Epoch 4, Batch 1300, Loss: 0.636\n",
            "Epoch 4, Batch 1400, Loss: 0.678\n",
            "Epoch 4, Batch 1500, Loss: 0.682\n",
            "Epoch 5, Batch 100, Loss: 0.525\n",
            "Epoch 5, Batch 200, Loss: 0.552\n",
            "Epoch 5, Batch 300, Loss: 0.525\n",
            "Epoch 5, Batch 400, Loss: 0.557\n",
            "Epoch 5, Batch 500, Loss: 0.578\n",
            "Epoch 5, Batch 600, Loss: 0.553\n",
            "Epoch 5, Batch 700, Loss: 0.555\n",
            "Epoch 5, Batch 800, Loss: 0.591\n",
            "Epoch 5, Batch 900, Loss: 0.567\n",
            "Epoch 5, Batch 1000, Loss: 0.576\n",
            "Epoch 5, Batch 1100, Loss: 0.572\n",
            "Epoch 5, Batch 1200, Loss: 0.612\n",
            "Epoch 5, Batch 1300, Loss: 0.574\n",
            "Epoch 5, Batch 1400, Loss: 0.584\n",
            "Epoch 5, Batch 1500, Loss: 0.575\n",
            "Akurasi: 72.58%\n"
          ]
        }
      ]
    }
  ]
}